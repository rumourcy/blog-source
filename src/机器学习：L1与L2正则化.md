---
title: 机器学习：L1与L2正则化
date: 2019-06-10 20:42:20
tags:
	- ZERO
	- Machine Learning
---

### L1与L2正则化形式

- L1正则化
  $$
  obj = \sum_{i}{l(y_{i},f(x_{i};w))} + \lambda\left\lVert w \right\rVert_{1}
  $$

- L2正则化
  $$
  obj = \sum_{i}{l(y_{i}, f(x_{i};w))} + \lambda\left\lVert w \right\rVert_{2}^{2}
  $$

<!--more-->

### L1与L2正则化区别

#### L1容易产生稀疏解

- 优化视角

  ![lp-norm](https://github.com/trierbo/blog-source/raw/master/pics/ml/lp-norm.png)

  <center>Lp-Norm几何图形</center>
目标函数可以改写成带约束条件的优化问题
  $$
  \min \sum_{i}{l(y_{i}, f(x_{i};w))} + \lambda\left\lVert w \right\rVert_{q}^{q} \to \\
  \min \sum_{i}{l(y_{i}, f(x_{i};w))} \\
  s.t.\ \left\lVert w \right\rVert_{q}^{q} \leq \eta
  $$
  ![lp-Lagrange](https://github.com/trierbo/blog-source/raw/master/pics/ml/lp-Lagrange.png)
  
<center>L1稀疏解</center>
  从上图可知，当峰值位于坐标轴附近时，L1都可以产生稀疏解

- 梯度视角
  $$
  obj = L + \lambda \left\lVert w \right\rVert_{1} \\
  \frac{\partial{obj}}{\partial{w_{i}}} = \frac{\partial{L}}{\partial{w_{i}}} + \lambda sign(w_{i}) \\
  sign(w_{i}) = \begin{cases}
  1 \quad w_{i} \gt 0 \\
  0 \quad w_{i} = 0 \\
  -1 \quad w_{i} \lt 0
  \end{cases}
  $$

  $$
  obj = L + \lambda \left\lVert w \right\rVert_{2}^{2} \\
  \frac{\partial{obj}}{\partial{w_{i}}} = \frac{\partial{L}}{\partial{w_{i}}} + 2\lambda w_{i}
  $$

  当$w_{i}$接近0时，假设$w_{i}$为正，L1仍会使$w_{i}$减小，直至为0；而L2很难使$w_{i}$继续减小，所以不能产生稀疏解 

#### L1不可导，L2可导

- 坐标下降法

  - 目标函数
    $$
    obj(w) = L(w) + \lambda \left\lVert w \right\rVert_{1}
    $$
    
  - 初始参数
    $$
    w^{0} = (w_{1}^{0}, w_{2}^{0}, \cdots, w_{m}^{0})
    $$

  - 对$k=1,2,...,K$
    $$
    w_{1}^{k} = \arg\min_{w_{1}} obj(w_{1}, w_{2}^{k-1}, \cdots, w_{m}^{k-1}) \\
    w_{2}^{k} = \arg\min_{w_{2}} obj(w_{1}^{k}, w_{2}, \cdots, w_{m}^{k-1}) \\
    \vdots\\
    w_{m}^{k} = \arg\min_{w_{m}} obj(w_{1}^{k}, w_{2}^{k}, \cdots, w_{m})
    $$

  - 重复上一步，直到$w$收敛

- 近端梯度下降
  $$
  \min f(x) + h(x)
  $$
  其中$f(x)$可导，$h(x)$不可导
  $$
  x^{k+1} = Prox_{h, \eta}(x^{k} - \eta \nabla f(x^{k})) \\
  Prox_{h, \eta}(x) = \arg\min_{y} \frac{1}{2 \eta} \left\lVert y - x \right\rVert^{2} + h(y)
  $$
  `proximal operator`公式寻找一个$x$的附近点$y$，使得$h(y)$尽可能小

  对于L1正则化不可导问题：
  $$
  h(x) = \lambda \left\lVert x \right\rVert_{1} \\
  Prox_{h, \eta}(x) = \arg\min_{y} \frac{1}{2 \eta} \left\lVert y - x \right\rVert^{2} + \lambda \left\lVert y \right\rVert_{1} = \hat{y}
  $$
  由于向量各个分量不交叉，所以对每个分量可单独求解
  $$
  \hat{y_{i}} = \arg\min_{y_{i}} \frac{1}{2 \eta} (y_{i} - x_{i})^{2} + \lambda \left\lvert y_{i} \right\rvert \\
  \hat{y_{i}} = \begin{cases}
  x_{i} - \lambda \eta \quad x_{i} - \lambda \eta \gt 0 \\
  x_{i} + \lambda \eta \quad x_{i} + \lambda \eta \lt 0 \\
  0 \quad otherwise
  \end{cases}
  $$

### L1与L2作为损失函数

- L1-Norm 损失函数，又被称为 **least absolute deviation (LAD，最小绝对偏差)**
  $$
  L = \sum_{i} \left\lvert y_{i} - \hat{y_{i}} \right\rvert
  $$

- L2-Norm 损失函数，又被称为**最小二乘误差 (least squares error, LSE)**
  $$
  L = \sum_{i} {(y_{i} - \hat{y_{i}})}^{2}
  $$

| L1 loss function       | L2 LOSS FUNCTION   |
| ---------------------- | ------------------ |
| ROBUST(对异常点不敏感) | NOT ROBUST         |
| UNIQUE SOLUTION        | MULTIPLE SOLUTIONS |

### Reference

Pattern Recognition and Machine Learning (chapter 3.1.4)

[l1 相比于 l2 为什么容易获得稀疏解?](https://www.zhihu.com/question/37096933)

[L1范数的最优化过程是怎么样的? 梯度下降遇到不可导点怎么办?](https://www.zhihu.com/question/38426074)

[近端梯度下降——Proximal Method](http://roachsinai.github.io/2016/08/03/1Proximal_Method/#proximal-gradient-method)

[Differences between L1 and L2 as Loss Function and Regularization](https://link.zhihu.com/?target=http%3A//www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)

[Why does the L2 norm loss have a unique solution and the L1 norm loss have possibly multiple solutions?](https://stats.stackexchange.com/questions/363144/why-does-the-l2-norm-loss-have-a-unique-solution-and-the-l1-norm-loss-have-possi)