---
title: 机器学习：优化算法
date: 2019-06-10 20:42:20
tags:
	- ZERO
	- Machine Learning

---

### 目标函数

$f_{i}(x)$表示第$i$个样本的损失函数，则目标函数可表示为：
$$
f(x) = \frac{1}{n} \sum_{1}^{n} f_{i}(x)
$$

### 梯度下降(Gradient Descent)

#### 梯度下降原理

$$
f(x + \epsilon) \approx f(x) + \langle\nabla f(x), \epsilon\rangle \quad \epsilon \in \mathring{U}(x) \\
\epsilon = -\eta \nabla f(x) \\
f(x + \epsilon) \approx f(x) + \langle\nabla f(x), -\eta \nabla f(x)\rangle \to \\
f(x + \epsilon) \lt f(x)
$$

#### 梯度下降

$$
x^{k+1} \gets x^{k} - \eta \nabla f(x^{k})
$$

#### 随机梯度下降

$$
x^{k+1} \gets x^{k} - \eta \nabla f_{i}(x)
$$

#### 小批量随机梯度下降

$$
g^{k+1} \gets \nabla f_{B^{k+1}}(x^{k}) = \frac{1}{\lvert B^{k+1} \rvert} \sum_{i \in B^{k+1}} \nabla f_{i}(x^{k}) \\
x^{k+1} \gets x^{k} - \eta g^{k+1}
$$

> 容易陷入局部最优或者鞍点

### 动量法



### AdaGrad

### RMSProp

### AdaDelta

### Adam

### Reference

[优化算法](http://zh.d2l.ai/chapter_optimization/index.html)