---
title: 线性代数：投影
date: 2019-04-22 20:58:22
tags:
  - Linear Algebra
---

### Orthogonal

![subspace](https://github.com/trierbo/blog-source/raw/master/pics/subspace/subspace.png)

<!--more-->

#### Orthogonal Vectors

$$
x^Ty = 0
$$

$$
\Vert x \Vert^{2} + \Vert y \Vert^{2} = \Vert x + y \Vert^{2} \to
x^{T}x + y^{T}y = (x+y)^{T}(x+y) = x^Tx+y^Ty+x^Ty+y^Tx \to
x^Ty = 0
$$

#### Subspace $S$ is orthogonal to subspace $T$

mean: every vector in $S$ is orthogonal to every vector in $T$

row space is orthogonal to null space

why?
$$
\left[\begin{array}{cccc}
row\ 1\ of\ A \\
row\ 2\ of\ A \\
\vdots\\
row\ m\ of\ A
\end{array}\right]
\left[\begin{array}{cccc}x\end{array}\right] = 
\left[\begin{array}{cccc}
0 \\
0 \\
\vdots \\
0
\end{array}\right]
$$

$$
c_{1}(row1)^Tx + c_{2}(row2)^Tx + \cdots + c_{m}(rowm)^Tx = 0 \to \\
(c_{1}row1 + c_{2}row_{2} + \cdots + c_{m}rowm)^Tx = 0
$$

null space and row space are orthogonal complements in $R^{n}$

null space($n-r$) contains all vectors $\bot$ row space($r$)

### Projection

Coming: $Ax = b$ when there is no solution

How to "solve"?

$$
N(A^TA) = N(A) \\
rank\ of\ A^TA = rank\ of\ A
$$
> $Ax = 0 \Rightarrow A^TAx = 0$
>
> $Ax = 0 \Rightarrow x^TA^TAx = 0 \Rightarrow (Ax)^TAx = 0 \Rightarrow Ax = 0$
>
> $N(A^TA) = N(A) \Leftrightarrow rank\ of\ A^TA = rank\ of\ A$

$A^TA$ is invertible exactly if $A$ has independent columns

> If $A$ has independent columns, then $A^TA$ is invertiable
>
> To prove:
>
> Suppose $A^TAx = 0$, $x$ must be $0$
>
> TRICK:
> $$
> x^TA^TAx = 0 \Rightarrow (Ax)^TAx = 0 \Rightarrow Ax = 0 \Rightarrow x = 0
> $$

#### Projection

![projection](https://github.com/trierbo/blog-source/raw/master/pics/projection/projection.png)

$$
a^T(b-xa) = 0 \Rightarrow xa^Ta = a^Tb \Rightarrow x = \frac{a^Tb}{a^Ta} \\
p = ax\\
p = a\frac{a^Tb}{a^Ta} = Pb\ in\ C(P) \\
P = \frac{aa^T}{a^Ta}(Projection\ matrix)
$$

- $C(P) = line\ through\ a$
  - $rank(P) = 1$
- $P^T = P$
- $P^2 = P$ 投影两次结果相同

#### Why project?

> Because $Ax = b$ many have no solution
>
> Solve $A\hat{x} = p$ instead, where $p$ is projection of $b$ onto col space!

![projection3d](https://github.com/trierbo/blog-source/raw/master/pics/projection/projection3D.png)

plane of $a_{1}$, $a_{2}$ = col space of $A = [a_{1}, a_{2}]$
$$
p = A\hat{x}
$$
Find $\hat{x}$
key: $b - A\hat{x}$ is prep to plane
$$
a_1^T(b-A\hat{x}) = 0 \\
a_2^T(b-A\hat{x}) = 0 \\
\left[\begin{array}{cccc}
a_1^T \\
a_2^T
\end{array}\right]
(b - A\hat{x}) = 
\left[\begin{array}{cccc}
0 \\
0
\end{array}\right] \\
A^T(b-A\hat{x}) = 0 \Rightarrow A^TA\hat{x} = A^Tb \\
e = b - p = b - A\hat{x}\ in\ N(A^T) \Rightarrow e \bot C(A)
$$
$$
\hat{x} = (A^TA)^{-1}A^Tb \\
p = A\hat{x} = A(A^TA)^{-1}A^Tb
$$
- matrix: $P = A(A^TA)^{-1}A^T$

- $P^T = P$
  $$
  A^{TT}(A(A^TA)^{-1})^T = A((A^TA)^{-1})^TA^T = A((A^TA)^T)^{-1}A^T = A(A^TA)^{-1}A^T
  $$

- $P^{2} = P$
  $$
  A(A^TA)^{-1}A^TA(A^TA)^{-1}A^{T}
  $$

### Least Squares

> fitting by a line

![least squares](https://github.com/trierbo/blog-source/raw/master/pics/projection/leastsquare.png)
$$
\begin{cases}
C + D = 1 \\
C + 2D = 2 \\
C + 3D = 2
\end{cases}
$$

$$
\left[\begin{array}{cccc}
1 & 1 \\
1 & 2 \\
1 & 3
\end{array}\right]
\left[\begin{array}{cccc}
C \\
D
\end{array}\right] = 
\left[\begin{array}{cccc}
1 \\
2 \\
2
\end{array}\right] \\
Ax = b
$$

Minimize $\Vert Ax - b \Vert^2 = \Vert e \Vert^{2} = e_1^2 + e_2^2 + e_3^2$

Find:
$$
\hat{x} =
\left[\
\begin{array}{cccc}
\hat{C} \\
\hat{D}
\end{array}\right] \quad p
$$

$$
A^TAx = A^Tb \\
\left[\begin{array}{cccc}
1 & 1 & 1 \\
1 & 2 & 3
\end{array}\right]
\left[\begin{array}{c:c}
\begin{matrix}
1 & 1 \\
1 & 2 \\
1 & 3
\end{matrix} &
\begin{matrix}
1 \\
2 \\
2
\end{matrix}
\end{array}\right] = 
\left[\begin{array}{c:c}
\begin{matrix}
3 & 6 \\
6 & 14
\end{matrix} &
\begin{matrix}
5 \\
11
\end{matrix}
\end{array}\right] \\
\begin{cases}
3C + 6D = 5 \\
6C + 14D = 11
\end{cases} \Rightarrow
\begin{cases}
C = \frac{2}{3} \\
D = \frac{1}{2}
\end{cases} \\
p(\frac{7}{6}, \frac{5}{3}, \frac{13}{6})
$$

Best line:
$$
b = \frac{2}{3} + \frac{1}{2}t
$$

#### Projection Matrix

$$
P = A(A^TA)^{-1}A^T
$$

- if $b$ in column space($b = Ax$), $Pb = b$
  $$
  p = Pb = A(A^TA)^{-1}A^TAx = Ax = b
  $$

- if $b \bot$ column space, $Pb = 0$

  $b$ in $N(A^T)$

  $p = Pb = A(A^TA)^{-1}A^Tb = 0$

![pe](https://github.com/trierbo/blog-source/raw/master/pics/projection/e.png)
$$
e \bot col\ space \Rightarrow e\ in\ N(A^T) \\
p + e = b \\
p = Pb \Rightarrow e = (I - P)b \quad projection\ onto\ the \bot space
$$

### Orthogonal Basis

> Columns definitely independent if they are perp unit vectors(orthonormal vectors)

Orthogonal basis: $q_1, \cdots, q_n$

orthonormal vectors
$$
q_{i}^{T}q_{j} =
\begin{cases}
0 \quad if\ i \neq j \\
1 \quad if\ i = j
\end{cases}
$$

$$
Q = 
\left[\begin{array}{cccc}
\vdots& \cdots& \vdots\\
q_{1}& \cdots& q_{n}\\
\vdots& \cdots& \vdots
\end{array}\right] \\
Q^TQ = 
\left[\begin{array}{cccc}
\cdots& q_{1}^{T}& \cdots\\
\vdots& \vdots& \vdots\\
\cdots& q_{n}^{T}& \cdots
\end{array}\right]
\left[\begin{array}{cccc}
\vdots& \cdots& \vdots\\
q_{1}& \cdots& q_{n}\\
\vdots& \cdots& \vdots
\end{array}\right] =
I
$$

> If $Q$ is square then $Q^TQ = I$, tell us $Q^T = Q^{-1}$

$Q$ has orthonormal columns, project onto its column space
$$
P = Q(Q^TQ)^{-1}Q^{T} = QQ^{T} \overset{Q\ is\ aquare}{=} I
$$

$$
AA^T\hat{x} = A^Tb
$$

when $A$ is $Q$\
$$
Q^TQ\hat{x} = Q^Tb \Rightarrow \hat{x} = Q^Tb \\
\hat{x_i} = q_{i}^{T}b
$$

#### Gram-Schmidt

- independent vectors $a$, $b$ $\to$

orthogonal $A$, $b$ $\to$

orthonormal $q_{1} = \frac{A}{\Vert A \Vert}$ $q_{2} = \frac{B}{\Vert B \Vert}$
$$
A = a\\
B = b - p = b - \frac{A^Tb}{A^TA}A \\
A \bot B \\
A^TB = A^T(b - \frac{A^Tb}{A^TA}A) = 0
$$
Example:
$$
a = \left[\begin{array}{cccc}
1 \\
1 \\
1
\end{array}\right] \quad
b = \left[\begin{array}{cccc}
1 \\
0 \\
2
\end{array}\right] \to \\
A = \left[\begin{array}{cccc}
1 \\
1 \\
1
\end{array}\right] \quad
B = \left[\begin{array}{cccc}
1 \\
0 \\
2
\end{array}\right] - \frac{3}{3}\left[\begin{array}{cccc}
1 \\
1 \\
1
\end{array}\right] =
\left[\begin{array}{cccc}
0 \\
-1 \\
1
\end{array}\right] \to \\
q_{1} = 
\left[\begin{array}{cccc}
\frac{1}{\sqrt{3}} \\
\frac{1}{\sqrt{3}} \\
\frac{1}{\sqrt{3}}
\end{array}\right] \quad
q_{2} =
\left[\begin{array}{cccc}
0 \\
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{array}\right] \to \\
Q = \left[\begin{array}{cccc}
\frac{1}{\sqrt{3}} & 0 \\
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}}
\end{array}\right]
$$

- independent vectors $a$, $b$, $c$
  $$
  A = a\\
  B = b - p = b - \frac{A^Tb}{A^TA}A \\
  C = c - \frac{A^Tc}{A^TA}A - \frac{B^Tc}{B^TB}B \\
  C \bot A \quad C \bot B
  $$

#### QR分解

$$
A = QR \\
\left[\begin{array}{cccc}
\vdots& \vdots \\
a_{1}& a_{2} \\
\vdots& \vdots
\end{array}\right] = 
\left[\begin{array}{cccc}
\vdots& \vdots \\
q_{1}& q_{2} \\
\vdots& \vdots
\end{array}\right]
\left[\begin{array}{cccc}
a_{1}^{T}q_{1} & a_{2}^{T}q_{1} \\
a_{1}^{T}q_{2} & a_{2}^{T}q_{2}
\end{array}\right]
$$

