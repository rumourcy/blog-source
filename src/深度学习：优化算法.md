---
title: 深度学习：优化算法
date: 2019-06-10 20:42:20
tags:
	- ZERO
	- Deep Learning
---

### 目标函数

$f_{i}(x)$表示第$i$个样本的损失函数，则目标函数可表示为：
$$
f(x) = \frac{1}{n} \sum_{1}^{n} f_{i}(x)
$$

### 梯度下降(Gradient Descent)

#### 梯度下降原理

$$
f(x + \epsilon) \approx f(x) + \langle\nabla f(x), \epsilon\rangle \quad \epsilon \in \mathring{U}(x) \\
\epsilon = -\eta \nabla f(x) \\
f(x + \epsilon) \approx f(x) + \langle\nabla f(x), -\eta \nabla f(x)\rangle \to \\
f(x + \epsilon) \lt f(x)
$$

#### 梯度下降

$$
x^{k+1} \gets x^{k} - \eta \nabla f(x^{k})
$$

#### 随机梯度下降

$$
x^{k+1} \gets x^{k} - \eta \nabla f_{i}(x)
$$

#### 小批量随机梯度下降

$$
g^{k+1} \gets \nabla f_{B^{k+1}}(x^{k}) = \frac{1}{\lvert B^{k+1} \rvert} \sum_{i \in B^{k+1}} \nabla f_{i}(x^{k}) \\
x^{k+1} \gets x^{k} - \eta g^{k+1}
$$

> 容易陷入局部最优或者鞍点

### 动量法(Momentum)

> 梯度下降在遇到沟壑时容易发生震荡，故可以引入动量，加速在正确的方向下降并抑制震荡
>
> ![Gradient Descent](http://zh.d2l.ai/_images/chapter_optimization_momentum_1_1.svg)

动量法引入速度变量$v^{k}$，初始化时$v^{0} = 0$

$$
v^{k+1} \gets \gamma v^{k} + \eta g^{k+1} \\
x^{k+1} \gets x^{k} - v^{k+1}
$$

![Momentum](http://zh.d2l.ai/_images/chapter_optimization_momentum_5_1.svg)

#### 指数加权移动平均

给定超参数$0 \leq \gamma \lt 1$，当前时间步$t$的变量$y_{t}$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_{t}$的线性组合：
$$
y_{t} = \gamma y_{t-1} + (1-\gamma) x_{t}
$$
对$y_{t}$展开：
$$
y_{t} = (1-\gamma) x_{t} + \gamma y_{t-1} \\
= (1-\gamma)x_{t} + (1-\gamma)\gamma x_{t-1} + \gamma^{2} y_{t-2} \\
= (1-\gamma)x_{t} + (1-\gamma)\gamma x_{t-1} + (1-\gamma)\gamma^{2}x_{t-2} + \gamma^{3}y_{t-3} \\
\cdots
$$

$$
y_{t} = (1 - \gamma) \sum_{i=0}^{t-1}\gamma^{i} x_{t-i}
$$

离时间$t$越近的$x_{t}$权值越大，较远的项可被忽略

#### 指数加权移动平均与动量法

$$
v^{k+1} \to \gamma v^{k} + (1 - \gamma)(\frac{\eta}{1 - \gamma}g^{k+1})
$$

### Nesterov Accelerated Gradient

$$
v^{k+1} \gets \gamma v^{k} + \eta \nabla_{x}f_{B^{k+1}}(x^{k} - \gamma v^{k}) \\
x^{k+1} \gets x^{k} - v^{k+1}
$$



### AdaGrad

> 根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率

- 梯度$g^{k}$按元素平方的累加变量$s^{k}$

$$
s^{k + 1} \gets s^{k} + g^{k + 1} \odot g^{k + 1}
$$

- 将自变量每个元素的学习率通过按元素运算重新调整一下
  $$
  x^{k + 1} \gets x^{k} - \frac{\eta}{\sqrt{s^{k+1} + \epsilon}} \odot g^{k+1}
  $$

> 由于$s^{k}$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）；当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解

### RMSProp

- 梯度按元素平方做指数加权移动平均变量$s^{k}$
  $$
  s^{k + 1} \gets \gamma s^{k} + (1 - \gamma) g^{k + 1} \odot g^{k + 1}
  $$

- 将自变量每个元素的学习率通过按元素运算重新调整一下
  $$
  x^{k + 1} \gets x^{k} - \frac{\eta}{\sqrt{s^{k+1} + \epsilon}} \odot g^{k+1}
  $$

#### AdaGrad与RMSProp对比

![AdaGrad](http://zh.d2l.ai/_images/chapter_optimization_adagrad_1_1.svg)

<center>AdaGrad</center>
![RMSProp](http://zh.d2l.ai/_images/chapter_optimization_rmsprop_1_1.svg)

<center>RMSProp</center>
### AdaDelta

- 梯度$g^{k}$按元素平方的指数加权移动平均变量$s^{k}$，初始时被初始化为$0$
  $$
  s^{k + 1} \gets \gamma s^{k} + (1 - \gamma) g^{k + 1} \odot g^{k + 1}
  $$
  
- AdaDelta算法维护一个额外的状态变量$\Delta x^{k}$，初始化时被初始化为$0$
  $$
  g^{'}_{k + 1} \gets \sqrt{\frac{\Delta x^{k+1} + \epsilon}{s^{k + 1} + \epsilon}} \odot g^{k + 1}
  $$

- 更新自变量
  $$
  x^{k+1} \gets x^{k} - g^{'}_{k+1}
  $$

- $\Delta x^{k + 1}$记录自变量$g^{'}_{k+1}$按元素平方的指数加权移动平均
  $$
  \Delta x^{k+1} \gets \rho \Delta x^{k} + (1 - \rho)g^{'}_{k+1} \odot g^{'}_{k+1}
  $$

### Adam

> Adam算法使用了动量变量$v^{k}$和RMSProp算法中梯度按元素平方的指数加权移动平均变量$s^{k}$

- 动量变量$v^{k}$
  $$
  v^{k + 1} \gets \beta_{1} v^{k} + (1 - \beta_{1})g^{k+1}
  $$

- 梯度按元素平方的指数加权移动平均变量$s^{k}$
  $$
  s^{k+1} \gets \beta_{2} s^{k} + (1 - \beta_{2}) g^{k + 1} \odot g^{k + 1}
  $$
  
- 偏差修正
  $$
  v^{k} = (1 - \beta_{1})\sum_{i = 1}^{k} \beta_{1}^{k-i}g^{i} \\
  (1 - \beta_{1}) \sum_{i = 1}^{k} \beta_{1}^{k - i} = 1 - \beta_{1} + \beta_{1} - \beta_{1}^{2} + \beta_{1}^{2} - \beta_{1}^{3} + \cdots - \beta_{1}^{k} = 1 - \beta_{1}^{k}
  $$
  当$k$较小时，过去各时间步随机梯度权值之和会较小
  $$
  \hat{v}^{k+1} \gets \frac{v^{k+1}}{1 - \beta_{1}^{k+1}} \\
  \hat{s}^{k+1} \gets \frac{s^{k+1}}{1 - \beta_{2}^{k+1}}
  $$

- 迭代自变量
  $$
  x^{k + 1} \gets x^{k} - \frac{\eta}{\sqrt{\hat{s}^{k+1}} + \epsilon} v^{k + 1}
  $$


### Reference

[优化算法](http://zh.d2l.ai/chapter_optimization/index.html)

[自适应学习率调整：AdaDelta](https://www.cnblogs.com/neopenx/p/4768388.html)

