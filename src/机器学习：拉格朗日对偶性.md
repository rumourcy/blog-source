---
title: 机器学习：拉格朗日对偶性
date: 2019-05-05 19:42:26
tags:
  - Machine Learning
  - Support Vector Machine
  - ZERO
---

#### Lagrange对偶函数

##### 优化问题

$$
\min f_{0}(x) \\
s.t. \ f_{i}(x) \leq 0, \ i = 1, \cdots, m \\
h_{i}(x) = 0, \ i = 1, \cdots, p
$$

##### Lagrange函数

$$
L(x, \lambda, v) = f_{0}(x) + \sum_{i=1}^{m} \lambda_{i} f_{i}(x) + \sum_{i=1}^{p} v_{i}h_{i}(x)
$$

其中$\lambda_{i}$以及$v_{i}$称为**Lagrange乘子**

$\lambda$以及$v$称为**Lagrange乘子向量**

##### Lagrange对偶函数

$$
g(\lambda, v) = \min_{x}L(x, \lambda, v)
$$

> 对偶函数是一族关于$(\lambda, v)$的仿射函数的逐点下确界，所以即使原问题不是凸的，对偶函数也是凹函数（p77）

<!--more-->

#### 对偶可行

对偶函数构成原问题最优值$p^{*}$的下界：即对任意$\lambda \geq 0$和$v$下式成立：
$$
g(\lambda, v) \leq p^{*}
$$
证：假设$x'$是原问题可行解
$$
\lambda \geq 0 \to
\sum_{i=1}^{m} \lambda_{i}f_{i}(x') + \sum_{i=1}^{p}v_{i}h_{i}(x') \leq 0 \to \\
L(x', \lambda, v) = f_{0}(x') + \sum_{i=1}^{m} \lambda_{i}f_{i}(x') + \sum_{i=1}^{p}v_{i}h_{i}(x') \leq f_{0}(x') \to \\
g(\lambda, v) = \min_{x}L(x, \lambda, v) \leq L(x', \lambda, v) \leq f_{0}(x')
$$

> 当$\lambda \geq 0$时，$g(\lambda, v) \leq p^{*}$, **对偶可行**

#### Lagrange对偶问题

$$
\max g(\lambda, v) \\
s.t. \ \lambda \geq 0
$$

> 对偶问题是一个凸优化问题，对偶问题的凸性和原问题是否是凸优化问题无关

#### 强弱对偶性

假设对偶问题的最优值用$d^{*}$表示

##### 弱对偶性

$$
d^{*} \leq p^{*}
$$

##### 最优对偶间隙

$$
p^{*} - d^{*}
$$

##### 强对偶性

$$
d^{*} = p^{*}
$$

- 原问题是凸优化问题，强对偶性通常成立

#### 互补松弛性

假设强对偶性成立
$$
f(x^{*}) = g(\lambda^{*}, v^{*}) \\
= \min_{x}{(f_{0}(x) + \sum_{i=1}^{m} \lambda^{*}_{i}f_{i}(x) + \sum_{i=1}^{p}v^{*}_{i}h_{i}(x))} \\
\leq f_{0}(x^{*}) + \sum_{i=1}^{m} \lambda^{*}_{i}f_{i}(x^{*}) + \sum_{i=1}^{p}v^{*}_{i}h_{i}(x^{*}) \\
\leq f_{0}(x^{*})
$$

$$
\sum_{i=1}^{m} \lambda^{*}_{i}f_{i}(x^{*}) = 0 \to
\lambda^{*}_{i}f_{i}(x^{*}) = 0
$$

上述条件称为**互补松弛性**

#### KKT条件

> 如果强对偶性成立，那么原问题最优解和对偶问题最优解必须满足KKT条件

- 原问题可行解

$$
f_{i}(x^{*}) \leq 0 \\
h_{i}(x^{*}) = 0
$$

- 对偶可行

$$
\lambda^{*}_{i} \geq 0
$$

- 强对偶性

  - 互补松弛性
    $$
    \lambda^{*}_{i}f_{i}(x^{*})
    $$

  - $L(x, \lambda^{*}, v^{*})$在$x^{*}$取得最小值
    $$
    L(x^{*}, \lambda^{*}, v^{*}) = f(x^{*}) = g(\lambda^{*}, v^{*})
    $$

    $$
    \nabla f_{0}(x^{*}) + \sum_{i=1}^{m} \lambda^{*}_{i} \nabla f_{i}(x^{*}) + \sum_{i=1}^{p}v^{*}_{i} \nabla h_{i}(x^{*}) = 0
    $$

### Reference

[Convex Optimization – Boyd and Vandenberghe](http://web.stanford.edu/~boyd/cvxbook/)

[凸优化](https://book.douban.com/subject/21249088/)

